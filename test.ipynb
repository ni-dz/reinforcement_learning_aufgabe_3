{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be878a7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "from stable_baselines3 import PPO\n",
    "from stable_baselines3.common.env_util import make_vec_env"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df4848ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "env = make_vec_env(\"CarRacing-v3\", n_envs=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "557c0841",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = PPO(\"CnnPolicy\", env, verbose=1, tensorboard_log=\"./ppo_car_racing_tensorboard/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ed2ae1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.learn(total_timesteps=100_000)\n",
    "model.save(\"ppo_car_racing\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd04a125",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "from stable_baselines3 import PPO\n",
    "from stable_baselines3.common.env_util import make_vec_env\n",
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "\"\"\"\n",
    "Reinforcement Learning Projekt: PPO-Agent für CarRacing-v3 (Box2D)\n",
    "\n",
    "Dieses Notebook zeigt, wie ein RL-Agent mit PPO (Proximal Policy Optimization) trainiert wird,\n",
    "um die komplexe Aufgabe des autonomen Fahrens in der Box2D CarRacing-v3 Umgebung zu lösen.\n",
    "\n",
    "Das Notebook umfasst:\n",
    "1. Umgebungsvorbereitung\n",
    "2. PPO-Agenten-Konfiguration\n",
    "3. Training des Agenten\n",
    "4. Evaluation & Visualisierung\n",
    "5. Automatische Endlos-Ausführung zur Live-Demonstration\n",
    "\n",
    "Die Umgebung liefert pixelbasierte Beobachtungen (RGB-Bilder), welche mit einer CNN-Policy\n",
    "verarbeitet werden. Ziel des Agenten ist es, die Rennstrecke effizient abzufahren.\n",
    "\n",
    "Durch die Nutzung von Deep Reinforcement Learning mit Proximal Policy Optimization kann der Agent\n",
    "lernen, komplexe Navigationsstrategien zu entwickeln, um Hindernissen auszuweichen und Kurven\n",
    "optimal zu durchfahren.\n",
    "\n",
    "Die CarRacing-Umgebung ist dabei ideal für kontinuierliche Steuerungsaufgaben geeignet und bietet\n",
    "sowohl eine hohe Komplexität als auch eine gute Visualisierbarkeit des Lernprozesses.\n",
    "\"\"\"\n",
    "\n",
    "# Schritt 1: Umgebung vorbereiten\n",
    "# Wir verwenden die neueste CarRacing-v3 Umgebung von OpenAI Gym.\n",
    "# Diese Umgebung nutzt Box2D-Physik und ist eine Standard-Benchmark für RL in kontinuierlichen Steuerungsaufgaben.\n",
    "# Beobachtungen bestehen aus 96x96 RGB-Bildern, die den aktuellen Zustand der Strecke darstellen.\n",
    "# Der Aktionsraum ist kontinuierlich und umfasst Lenken, Gas geben und Bremsen.\n",
    "\n",
    "env = make_vec_env(\"CarRacing-v3\", n_envs=1)\n",
    "\n",
    "# Schritt 2: PPO-Agent mit CNN-Policy definieren\n",
    "# Der PPO-Agent nutzt eine Convolutional Neural Network Policy (CnnPolicy), um direkt aus Bilddaten zu lernen.\n",
    "# Dies ermöglicht es dem Agenten, visuelle Informationen effektiv zu verarbeiten und darauf basierend Handlungen abzuleiten.\n",
    "model = PPO(\n",
    "    \"CnnPolicy\",\n",
    "    env,\n",
    "    verbose=1,\n",
    "    tensorboard_log=\"./ppo_car_racing_tensorboard/\"\n",
    ")\n",
    "\n",
    "# Schritt 3: Training\n",
    "# Wir trainieren den Agenten für 500.000 Zeitschritte.\n",
    "# Hinweis: Das Training kann je nach Hardware einige Zeit in Anspruch nehmen.\n",
    "# In dieser Phase lernt der Agent durch Trial-and-Error, die Kontrolle über das Fahrzeug zu verbessern.\n",
    "# Rewards werden vergeben, wenn der Agent auf der Strecke bleibt und möglichst schnell fährt.\n",
    "\n",
    "print(\"\\n=== Starte Training des PPO-Agenten für CarRacing-v3 ===\")\n",
    "model.learn(total_timesteps=500_000)\n",
    "print(\"=== Training abgeschlossen ===\\n\")\n",
    "\n",
    "# Modell speichern\n",
    "model.save(\"ppo_car_racing\")\n",
    "print(\"Modell wurde gespeichert unter 'ppo_car_racing'\\n\")\n",
    "\n",
    "# Schritt 4: Modell laden und Evaluation durchführen\n",
    "# Hier zeigen wir, wie der Agent die Strecke befährt.\n",
    "# Das Rendering erfolgt live im OpenCV-Fenster, damit das Verhalten sichtbar wird.\n",
    "\n",
    "eval_env = gym.make(\"CarRacing-v3\", render_mode=\"rgb_array\")\n",
    "model = PPO.load(\"ppo_car_racing\")\n",
    "\n",
    "# Automatische Endlos-Ausführung zur Live-Demonstration\n",
    "# Der Agent fährt kontinuierlich, Episoden starten nach jedem Absturz neu.\n",
    "# Ziel: Live-Visualisierung des aktuellen Agentenverhaltens über mehrere Episoden hinweg.\n",
    "\n",
    "print(\"\\n=== Starte Endlos-Demo des trainierten Agenten ===\")\n",
    "\n",
    "while True:\n",
    "    obs = eval_env.reset()\n",
    "    done = False\n",
    "    total_reward = 0\n",
    "    steps = 0\n",
    "\n",
    "    while not done:\n",
    "        # Aktion vom Agenten vorhersagen\n",
    "        action, _ = model.predict(obs)\n",
    "        obs, reward, done, info = eval_env.step(action)\n",
    "        total_reward += reward\n",
    "        steps += 1\n",
    "\n",
    "        # Bild für OpenCV-Rendering vorbereiten\n",
    "        frame = eval_env.render()\n",
    "\n",
    "        # Zusätzliche HUD-Informationen ins Bild zeichnen\n",
    "        hud = np.zeros((40, frame.shape[1], 3), dtype=np.uint8)\n",
    "        text = f\"Steps: {steps} | Reward: {total_reward:.2f}\"\n",
    "        cv2.putText(hud, text, (10, 25), cv2.FONT_HERSHEY_SIMPLEX, 0.8, (255, 255, 255), 2)\n",
    "        combined = np.vstack([frame, hud])\n",
    "\n",
    "        # Bild anzeigen\n",
    "        cv2.imshow(\"Car Racing - Live Demo\", combined)\n",
    "\n",
    "        # Falls 'q' gedrückt wird, Programm beenden\n",
    "        if cv2.waitKey(20) & 0xFF == ord(\"q\"):\n",
    "            eval_env.close()\n",
    "            cv2.destroyAllWindows()\n",
    "            print(\"\\nDemo manuell beendet.\\n\")\n",
    "            exit()\n",
    "\n",
    "    # Nach jeder Episode Zusammenfassung ausgeben\n",
    "    print(f\"Episode abgeschlossen. Gesamt-Reward: {total_reward:.2f}, Schritte: {steps}\")\n",
    "    print(\"Starte neue Episode...\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38397eb4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
